import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import time
import csv

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.neural_network import MLPClassifier
from sklearn.externals import joblib

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from IPython.display import display


filename_datasets = 'Dataset_Final_10800.csv'
df_datasets = pd.read_csv(filename_datasets)
df_datasets = df_datasets.drop(['file_name'],axis=1)


df_datasets.duplicated()         #Understand which one is duplicared
df_datasets.drop_duplicates()    #Drop Duplicates



#basic plot
#plt.boxplot(df_datasets)

'''#notched plot
plt.figure()
plt.boxplot(df_datasets, 1)

# change outlier point symbols
plt.figure()
plt.boxplot(df_datasets, 0, 'gD')

# don't show outlier points
plt.figure()
plt.boxplot(df_datasets, 0, '')

# horizontal boxes
plt.figure()
plt.boxplot(df_datasets, 0, 'rs', 0)

# change whisker length
plt.figure()
plt.boxplot(df_datasets, 0, 'rs', 0, 0.75)


'''


print(df_datasets.shape)
display(df_datasets.head())
display(df_datasets.describe())

#Heatmap

correlation_matrix = df_datasets.corr()
plt.figure(figsize=(10,8))
print('Heatmap')
ax = sns.heatmap(correlation_matrix, vmax=1, square=True,annot=True,cmap='RdYlGn')
plt.title('Correlation matrix between the features')
plt.show()





def get_train_test(df, y_col, ratio):
    mask = np.random.rand(len(df)) < ratio
    df_train = df[mask]
    df_test = df[~mask]

    Y_train = df_train[y_col].values
    Y_test = df_test[y_col].values
    del df_train[y_col]
    del df_test[y_col]

    X_train = df_train.values
    X_test = df_test.values
    return X_train, Y_train, X_test, Y_test

y_col = 'label'
train_test_ratio = 0.7
X_train, Y_train, X_test, Y_test = get_train_test(df_datasets, y_col, train_test_ratio)




dict_classifiers = {
    "Logistic_Regression": LogisticRegression(),
    "Nearest_Neighbors": KNeighborsClassifier(),
    "Linear_SVM": SVC(),
    "Gradient_Boosting_Classifier": GradientBoostingClassifier(),
    "Decision_Tree": tree.DecisionTreeClassifier(),
    "Random_Forest": RandomForestClassifier(n_estimators = 18),
    "Neural_Net": MLPClassifier(alpha = 1),
    "Naive_Bayes": GaussianNB()
}

no_classifiers = len(dict_classifiers.keys())
def batch_classify(X_train, Y_train, X_test, Y_test, verbose = True):
    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,8)), columns = ['classifier', 'train_score', 'test_score', 'training_time','true negative','false positive','false negative','true positive'])
    count = 0
    for key, classifier in dict_classifiers.items():
        t_start = time.clock()
        classifier.fit(X_train, Y_train)
        classifierstored = classifier.predict(X_test)


        tn, fp, fn, tp = confusion_matrix(classifierstored, Y_test).ravel()


        #save the model to disk
        filename = key+"_finalized_model_10800.sav"
        joblib.dump(classifier, filename)

        t_end = time.clock()
        t_diff = t_end - t_start
        train_score = classifier.score(X_train, Y_train)
        test_score = classifier.score(X_test, Y_test)
        df_results.loc[count,'classifier'] = key
        df_results.loc[count,'train_score'] = train_score
        df_results.loc[count,'test_score'] = test_score
        df_results.loc[count,'training_time'] = t_diff

        df_results.loc[count,'true negative'] = tn
        df_results.loc[count,'false positive'] = fp
        df_results.loc[count,'false negative'] = fn
        df_results.loc[count,'true positive'] = tp

        if verbose:
            print("trained {c} in {f:.2f} s".format(c=key, f=t_diff))
        count+=1
    return df_results

df_results = batch_classify(X_train, Y_train, X_test, Y_test)





display(df_results.sort_values(by='test_score', ascending=False))



#load the model from disk
#loaded_model = joblib.load(filename)
#result = loaded_model.score(X_test, Y_test)
#print(result)


#---------------------------------------------------------------------------------------------------Train End---------------------------------------------------------------------------------------------------


#BoxPlot

import seaborn as sns
t1=[]
t1=df_datasets.columns
print(t1)
for i in range(1,506,1):
    sns.boxplot(x = 'label',y = t1[i],data = df_datasets)
    plt.legend()
    plt.show()
    plt.close()


#---------------------------------------------------------------------------------------------------Testing---------------------------------------------------------------------------------------------------
testFrame = pd.read_csv('blankDataFrame.csv')
#print (testFrame)


#testFrame = testFrame.drop("label",axis = 1)

read_ldata = open("somemanifestfile.ldata","r").read()
shakes = open("somemanifestfile.ldata","r")
shakes_2 = open("somemanifestfile.ldata","r")
shakes_3 = open("somemanifestfile.ldata","r")

#print (read_ldata)

all_permission = []

activity_list = read_ldata.count('ActivityList')
#print activity_list
service_list = read_ldata.count('ServiceList')
#print service_list
content_provider_list = read_ldata.count('ContentProviderList')
#print content_provider_list
broadcast_receiver_list = read_ldata.count('BroadcastReceiverList')
#print broadcast_receiver_list
intent_filter_list = read_ldata.count('IntentFilterList')
#print intent_filter_list

for whole_doc_container in shakes:
	#print whole_doc_container #fetching the whole document by a for loop
	if "RequestedPermissionList" in whole_doc_container: #finding all RequestedPermissionList in the whole document. Another way is re.match("RequestedPermissionList",whole_doc_container):
		#print whole_doc_container
		whole_doc_container = whole_doc_container.strip()
		if whole_doc_container.startswith("RequestedPermissionList_"):
			whole_doc_container = whole_doc_container[32:]
			#all_permission.append(whole_doc_container)
			if whole_doc_container.startswith("permission"):
				all_permission.append(whole_doc_container) #adding the android.permission values in the list all_permission. the coloumns are added AUTOMATICALLY. WHERE IT IS ADDED AUTOMATICALLY??
				if whole_doc_container not in list(testFrame.columns.values):#checking which android.permission is not in the coloumns
					#print whole_doc_container
					testFrame[whole_doc_container]=0#assigning its value to 0
shakes.close()

for whole_doc_container_2 in shakes_2:
	#print shakes_2
	#print whole_doc_container_2 #fetching the whole document by a for loop
	if "HardwareComponentsList" in whole_doc_container_2: #finding all android.permissions in the whole document. Another way is re.match("RequestedPermissionList",whole_doc_container):
		#print whole_doc_container_2
		whole_doc_container_2 = whole_doc_container_2.strip()
		if whole_doc_container_2.startswith("HardwareComponentsList"):
			whole_doc_container_2 = whole_doc_container_2[31:]
			all_permission.append(whole_doc_container_2) #adding the android.permission values in the list all_permission. the coloumns are added AUTOMATICALLY. WHERE IT IS ADDED AUTOMATICALLY??
			if whole_doc_container_2 not in list(testFrame.columns.values):#checking which android.permission is not in the coloumns
				#print whole_doc_container_2
				testFrame[whole_doc_container_2]=0#assigning its value to 0
shakes_2.close()

custom_permissions = 0
for whole_doc_container_3 in shakes_3:
    #print whole_doc_container
    if "RequestedPermissionList" in whole_doc_container_3:
        whole_doc_container_3 = whole_doc_container_3.strip()
        if whole_doc_container_3.startswith("RequestedPermissionList"):
            whole_doc_container_3 = whole_doc_container_3[32:]
            #print whole_doc_container
            if not whole_doc_container_3.startswith("permission"):
                #print whole_doc_container
                custom_permissions = len(whole_doc_container_3)
                #print custom_permissions



data = [activity_list, service_list, content_provider_list, broadcast_receiver_list, intent_filter_list, custom_permissions] + [1 if p in all_permission else 0 for p in list(testFrame.columns.values)[6:]]

testFrame.loc[len(testFrame)] = data






#loaded_model.fit(X_test,Y_test)


target = []

'''"Logistic_Regression":
    "Nearest_Neighbors":
    "Linear_SVM":
    "Gradient_Boosting_Classifier"
    "Decision_Tree":
    "Random_Forest":
    "Neural_Net":
    "Naive_Bayes'''

#load the model from disk
loaded_model   = joblib.load('Neural_Net_finalized_model_10800.sav')
loaded_model_1 = joblib.load('Nearest_Neighbors_finalized_model_10800.sav')
loaded_model_2 = joblib.load('Random_Forest_finalized_model_10800.sav')
loaded_model_3 = joblib.load('Naive_Bayes_finalized_model_10800.sav')
loaded_model_4 = joblib.load('Logistic_Regression_finalized_model_10800.sav')
loaded_model_5 = joblib.load('Gradient_Boosting_Classifier_finalized_model_10800.sav')
loaded_model_6 = joblib.load('Decision_Tree_finalized_model_10800.sav')
loaded_model_7 = joblib.load('Linear_SVM_finalized_model_10800.sav')

print ('Neural Net')
result = loaded_model.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target=loaded_model.predict(testFrame)
print(target)



print ('Nearest Neighbours')
result_1 = loaded_model_1.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_1=loaded_model_1.predict(testFrame)
print(target_1)


print ('Random_Forest')
result_2 = loaded_model_2.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_2=loaded_model_2.predict(testFrame)
print(target_2)


print ('Naive_Bayes')
result_3 = loaded_model_3.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_3=loaded_model_3.predict(testFrame)
print(target_3)


print ('Logistic Regression')
result_4 = loaded_model_4.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_4=loaded_model_4.predict(testFrame)
print(target_4)



print ('Gradient Boosting')
result_5 = loaded_model_5.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_5=loaded_model_5.predict(testFrame)
print(target_5)



print ('Decision Tree')
result_6 = loaded_model_6.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_6=loaded_model_6.predict(testFrame)
print(target_6)



print ('Linear SVM')
result_7 = loaded_model_7.score(X_test, Y_test)
#loaded_model.fit(train_data, target)
#testFrame=testFrame.drop(['label'])
target_7=loaded_model_7.predict(testFrame)
print(target_7)
